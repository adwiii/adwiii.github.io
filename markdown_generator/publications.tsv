pub_date	title	venue	excerpt	citation	url_slug	paper_url	bibtex	image	github	short_venue
2019-6-24	Faster Biclique Mining in Near-Bipartite Graphs	International Symposium on Experimental Algorithms	Identifying dense bipartite subgraphs is a common graph data mining task. Many applications focus on the enumeration of all maximal bicliques (MBs), though sometimes the stricter variant of maximal induced bicliques (MIBs) is of interest. Recent work of Kloster et al. introduced a MIB-enumeration approach designed for ‚Äúnear-bipartite‚Äù graphs, where the runtime is parameterized by the size k of an odd cycle transversal (OCT), a vertex set whose deletion results in a bipartite graph. Their algorithm was shown to outperform the previously best known algorithm even when k was logarithmic in <code>|V|</code>. In this paper, we introduce two new algorithms optimized for near-bipartite graphs - one which enumerates MIBs in time <code>O(M<sub>I</sub>|V||E|k)</code>, and another based on the approach of Alexe et al. which enumerates MBs in time <code>O(M<sub>B</sub>|V||E|k)</code>, where <code>M<sub>I</sub></code> and <code>M<sub>B</sub></code> denote the number of MIBs and MBs in the graph, respectively. We implement all of our algorithms in open-source C++ code and experimentally verify that the OCT-based approaches are faster in practice than the previously existing algorithms on graphs with a wide variety of sizes, densities, and OCT decompositions.	"Sullivan, Blair D., Andrew van der Poel, and Trey Woodlief. ""Faster Biclique Mining in Near-Bipartite Graphs."" <i>International Symposium on Experimental Algorithms</i>. Springer, Cham, 2019."	faster-biclique-mining	https://link.springer.com/chapter/10.1007/978-3-030-34029-2_28	@inproceedings{sullivan2019faster,  title={Faster Biclique Mining in Near-Bipartite Graphs},  author={Sullivan, Blair D and Poel, Andrew van der and Woodlief, Trey},  booktitle={International Symposium on Experimental Algorithms},  pages={424--453},  year={2019},  organization={Springer}}			SEA^2'19
2021-5-30	Fuzzing Mobile Robot Environments for Fast Automated Crash Detection	2021 IEEE International Conference on Robotics and Automation (ICRA)	Testing mobile robots is difficult and expensive, and many faults go undetected. In this work we explore whether fuzzing, an automated test input generation technique, can more quickly find failure inducing inputs in mobile robots. We developed a simple fuzzing adaptation, BASE-FUZZ, and one specialized for fuzzing mobile robots, PHYS-FUZZ. PHYS-FUZZ is unique in that it accounts for physical attributes such as the robot dimensions, estimated trajectories, and time to impact measures to guide the test input generation process. The results of evaluating PHYS-FUZZ suggest that it has the potential to speed up the discovery of input scenarios that reveal failures, finding 56.5% more than uniform random input selection and 7.0% more than BASE-FUZZ during 7 days of testing.	"T. Woodlief, S. Elbaum and K. Sullivan, ""Fuzzing Mobile Robot Environments for Fast Automated Crash Detection,"" <i>2021 IEEE International Conference on Robotics and Automation (ICRA)</i>, 2021, pp. 5417-5423, doi: 10.1109/ICRA48506.2021.9561627."	fuzzing-mobile-robot-environments	https://ieeexplore.ieee.org/abstract/document/9561627	@inproceedings{woodlief2021fuzzing,  title={Fuzzing Mobile Robot Environments for Fast Automated Crash Detection},  author={Woodlief, Trey and Elbaum, Sebastian and Sullivan, Kevin},  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},  pages={5417--5423},  year={2021},  organization={IEEE}}	/images/icra_fuzzing.png		ICRA'21
2022-5-29	Preparing Software Engineers to Develop Robot Systems	44th International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET ‚Äô22)	Robotics is a rapidly expanding field that needs software engineers. Most of our undergraduates, however, are not equipped to manage the unique challenges associated with the development of software for modern robots. In this work we introduce a course we have designed and delivered to better prepare students to develop software for robot systems. The course is unique in that: it emphasizes the distinctive challenges of software development for robots paired with the software engineering techniques that may help manage those challenges, it provides many opportunities for experiential learning across the robotics and software engineering interface, and it lowers the barriers for learning how to build such systems. In this work we describe the principles and innovations of the course, its content and delivery, and finish with the lessons we have learned"	Carl Hildebrandt, Meriel von Stein, Trey Woodlief, and Sebastian Elbaum. 2022. Preparing Software Engineers to Develop Robot Systems. In 44th International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET ‚Äô22), May 21‚Äì29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3510456.3514161	preparing-software-engineers-to-develop-robot-systems	https://hildebrandt-carl.com/files/2022-05-21-ICSE-SERobotics.pdf	@INPROCEEDINGS{hildebrandt2022preparing,  author={Hildebrandt, Carl and Stein, Meriel von and Woodlief, Trey and Elbaum, Sebastian},  booktitle={2022 IEEE/ACM 44th International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)},   title={Preparing Software Engineers to Develop Robot Systems}, year={2022},  pages={205-216},  doi={10.1145/3510456.3514161}}	/images/cs4501_fall22_project.gif		ICSE-SEET'22
2022-5-29	Semantic Image Fuzzing of AI Perception Systems	44th International Conference on Software Engineering (ICSE 2022)	Perception systems enable autonomous systems to interpret raw sensor readings of the physical world. Testing of perception systems aims to reveal misinterpretations that could cause system failures. Current testing methods, however, are inadequate. The cost of human interpretation and annotation of real-world input data is high, so manual test suites tend to be small. The simulation-reality gap reduces the validity of test results based on simulated worlds. And methods for synthesizing test inputs do not provide corresponding expected interpretations. To address these limitations, we developed ùë†ùëíùëöùëÜùëíùëõùë†ùêπùë¢ùëßùëß, a new approach to fuzz testing of perception systems based on semantic mutation of test cases that pair real-world sensor readings with their ground-truth interpretations. We implemented our approach to assess its feasibility and potential to improve software testing for perception systems. We used it to generate 150,000 semantically mutated image inputs for five state-of-the-art perception systems. We found that it synthesized tests with novel and subjectively realistic image inputs, and that it discovered inputs that revealed significant inconsistencies between the specified and computed interpretations. We also found that it produced such test cases at a cost that was very low compared to that of manual semantic annotation of real-world images.	Trey Woodlief, Sebastian Elbaum, and Kevin Sullivan. 2022. Semantic Image Fuzzing of AI Perception Systems. In 44th International Conference on Software Engineering (ICSE ‚Äô22), May 21‚Äì29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3510003.3510212	semantic-image-fuzzing	https://github.com/less-lab-uva/perception_fuzzing/blob/main/Semantic%20Image%20Fuzzing%20of%20AI%20Perception%20Systems.pdf	@inproceedings{woodlief2022semantic,  title={Semantic image fuzzing of AI perception systems},  author={Woodlief, Trey and Elbaum, Sebastian and Sullivan, Kevin},  booktitle={Proceedings of the 44th International Conference on Software Engineering}, pages={1958--1969}, year={2022}}	/images/semSensFuzz.png	https://github.com/less-lab-uva/perception_fuzzing	ICSE'22
2023-5-20	Generating Realistic and Diverse Tests for LiDAR-Based Perception Systems	45th International Conference on Software Engineering (ICSE 2023)	Autonomous systems rely on a perception component to interpret their  surroundings, and when   misinterpretations occur, they can and have led to serious and fatal system-level failures. Yet, existing methods for testing  perception software remain limited in both their capacity to efficiently generate test data that translates to real-world performance and in their diversity to capture the long tail of rare but safety-critical scenarios. These limitations are particularly evident for  perception systems based on LiDAR sensors, which have emerged as a crucial component in modern autonomous systems due to their ability to provide a 3D scan of the world and operate in all lighting conditions. To address these limitations, we introduce a novel approach for testing LiDAR-based perception systems by leveraging existing real-world data as a basis to generate realistic and diverse test cases through mutations that preserve realism invariants while generating inputs rarely found in existing data sets, and automatically crafting   oracles that identify potentially safety-critical issues in perception performance. We implemented our approach to assess its ability to identify perception failures, generating over 50,000 test inputs for five state-of-the-art LiDAR-based perception systems. We found that it efficiently generated test cases that  yield errors in perception that could result in real consequences if these systems were deployed and does so at a low rate of false positives.	Garrett Christian, Trey Woodlief, and Sebastian Elbaum. 2023. Generating Realistic and Diverse Tests for LiDAR-Based Perception Systems. In 45th International Conference on Software Engineering (ICSE ‚Äô23), May 17‚Äì19, 2023, Melbourne, VIC, AU. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1109/ICSE48619.2023.00217	semantic-lidar-fuzzing	https://github.com/less-lab-uva/semLidarFuzz/blob/master/Generating%20Realistic%20and%20Diverse%20Tests%20for%20LiDAR-Based%20Perception%20Systems.pdf	@inproceedings{christian2023generating,title={Generating Realistic and Diverse Tests for LiDAR-Based Perception Systems},author={Christian$^*$, Garrett and Woodlief, Trey and Elbaum, Sebastian},booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},pages={2604--2616},year={2023},organization={IEEE}}	/images/semLidarFuzz.png	https://github.com/less-lab-uva/semLidarFuzz	ICSE'23
2024-4-20	S<sup>3</sup>C: Spatial Semantic Scene Coverage for Autonomous Vehicles	46th International Conference on Software Engineering (ICSE 2024)	"Autonomous vehicles (AVs)  must be able to operate in a wide range of scenarios including those in the <i>long tail</i> distribution that include rare but safety-critical events.
The collection of sensor input and expected output datasets from such scenarios is crucial for the development and testing of such systems.
Yet, approaches to quantify the extent to which a dataset covers test specifications that capture critical scenarios remain limited in their ability to discriminate between inputs that lead to distinct behaviors, and to render interpretations that are relevant to AV domain experts.
To address  this challenge, we introduce S<sup>3</sup>C, a framework that abstracts sensor inputs to coverage domains that account for the <i>spatial semantics</i> of a scene. The approach
leverages scene graphs to produce a sensor-independent abstraction of the AV environment that is interpretable and discriminating.
We provide an implementation of the approach and a study for camera-based autonomous vehicles operating in simulation. The findings show that S<sup>3</sup>C outperforms existing techniques in  discriminating among classes of inputs that cause failures, and offers spatial interpretations that can explain to what extent a dataset covers a test specification. Further exploration of S<sup>3</sup>C with open datasets complements the study findings,  revealing the potential and shortcomings of deploying the approach in the wild."	Trey Woodlief, Felipe Toledo, Sebastian Elbaum, and Matthew B. Dwyer. 2024. S3C: Spatial Semantic Scene Coverage for Autonomous Vehicles. In 2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE ‚Äô24), April 14‚Äì20, 2024, Lisbon, Portugal. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3597503.3639178	s3c	https://github.com/less-lab-uva/s3c/blob/main/S3C%20Spatial%20Semantic%20Scene%20Coverage%20for%20Autonomous%20Vehicles.pdf	@inproceedings{woodlief2024s3c,title={S3C: Spatial Semantic Scene Coverage for Autonomous Vehicles},author={Woodlief, Trey and Toledo, Felipe and Elbaum, Sebastian and Dwyer, Matthew B.},booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE ‚Äô24)},year={2024},organization={ACM}}	/images/s3c_fig_test_specs.png	https://github.com/less-lab-uva/s3c	ICSE'24
2024-5-17	Specifying and Monitoring Safe Driving Properties with Scene Graphs	"2024 IEEE International Conference on
Robotics and Automation"	With the proliferation of autonomous vehicles (AVs)  comes the need to ensure they abide to safe driving  properties. Specifying and monitoring such properties, however, is challenging because of the mismatch between the semantic space over which typical driving properties are asserted (e.g., vehicles,   pedestrians, intersections) and the sensed inputs of AVs. Existing efforts either assume for such sematic data  to be available or develop bespoke methods for capturing it. Instead, this work introduces a framework that can extract scene graphs  from sensor inputs to capture the entities related to the AV, and a domain-specific language  that enables   building propositions over those  graphs and composing them through temporal logic. We implemented the framework to monitor for specification violations of 3 top AVs from the CARLA Autonomous Driving Leaderboard, and found that on average the AVs violated 71% of properties during at least one test.	"Toledo, Felipe, Trey Woodlief, Sebastian Elbaum, and Matthew B. Dwyer. ""Specifying and Monitoring Safe Driving Properties with Scene Graphs."" In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 15577-15584. IEEE, 2024."	sg-monitoring	https://github.com/less-lab-uva/ExtendingSGSM/blob/master/ICRA24_Specifying_and_Monitoring_with_Scene_Graphs.pdf	@inproceedings{toledo2024specifying,title={Specifying and Monitoring Safe Driving Properties with Scene Graphs},author={Toledo, Felipe and Woodlief, Trey and Elbaum, Sebastian and Dwyer, Matthew B},booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},pages={15577--15584},year={2024},organization={IEEE}}	/images/icra24.png	https://github.com/less-lab-uva/sgsm	ICRA'24
2024-10-18	ODD-diLLMma: Driving Automation System ODD Compliance Checking using LLMs	2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS'24)	Although Driving Automation Systems (DASs) are rapidly becoming more advanced and ubiquitous, they are still confined to specific Operational Design Domains (ODDs) over which the system must be trained and validated. Yet, each DAS has a bespoke and often informally defined ODD, which makes it intractable to manually judge whether a dataset satisfies a DAS‚Äôs ODD. This results in inadequate data leaking into the training and testing processes, weakening them, and causes large amounts of collected data to go unused given the inability to check their ODD compliance. This presents a dilemma: How do we cost-effectively determine if existing sensor data complies with a DAS‚Äôs ODD? To address this challenge, we start by reviewing the ODD specifications of 10 commercial DASs to understand current practices in ODD documentation. Next, we present ODD-diLLMma, an automated method that leverages Large Language Models (LLMs) to analyze existing datasets with respect to the natural language specifications of ODDs. Our evaluation of ODD-diLLMma examines its utility in analyzing inputs from 3 real-world datasets. Our empirical findings show that ODD-diLLMma significantly enhances the efficiency of detecting ODD compliance, showing improvements of up to 147% over a human baseline. Further, our analysis highlights the strengths and limitations of employing LLMs to support ODD-diLLMma, underscoring their potential to effectively address the challenges of ODD compliance detection.	Coming Soon	odd-dillmma	https://github.com/hildebrandt-carl/ODD_diLLMma_Artifact/blob/master/PaperArtifact/ODD_diLLMma_Paper.pdf	Coming Soon	/images/odd-dillmma.png	https://github.com/hildebrandt-carl/ODD_diLLMma_Artifact	IROS'24
2025-04-30	A Differential Testing Framework to Identify Critical AV Failures Leveraging Arbitrary Inputs	47th International Conference on Software Engineering (ICSE 2025)	The proliferation of autonomous vehicles (AVs) has made their failures increasingly evident. Testing efforts aimed at identifying the inputs leading to those failures are challenged by the input‚Äôs long-tail distribution, whose area under the curve is dominated by rare scenarios. We hypothesize that leveraging emerging open-access datasets can accelerate the exploration of long-tail inputs. Having access to diverse inputs, however, is not sufficient to expose failures; an effective test also requires an oracle to distinguish between correct and incorrect behaviors. Current datasets lack such oracles and developing them is notoriously difficult. In response, we propose DIFFTEST4AV, a differential testing framework designed to address the unique challenges of testing AV systems: 1) for any given input, many outputs may be considered acceptable, 2) the long-tail contains an insurmountable number of inputs to explore, and 3) the AV‚Äôs continuous execution loop requires for failures to persist in order to affect the system. DIFFTEST4AV integrates statistical analysis to identify meaningful behavioral variations, judges their importance in terms of the severity of these differences, and incorporates sequential analysis to detect persistent errors indicative of potential system-level failures. Our study on 5 versions of the commercially-available, road-deployed comma.ai OpenPilot system, using 3 available image datasets, demonstrates the capabilities of the framework to detect high-severity, high-confidence, long-running test failures.	Coming Soon	difftest4av	/files/Differential_Testing_ICSE_2025.pdf	Coming Soon	/images/difftest4av_splash.png		ICSE'25
2024-11-26	The SGSM framework: Enabling the specification and monitor synthesis of safe driving properties through scene graphs	Science of Computer Programming	As autonomous vehicles (AVs) become mainstream, assuring that they operate in accordance with safe driving properties becomes paramount. The ability to specify and monitor driving properties is at the center of such assurance. Yet, the mismatch between the semantic space over which typical driving properties are asserted (e.g., vehicles, pedestrians) and the sensed inputs of AVs (e.g., images, point clouds) poses a significant assurance gap. Related efforts bypass this gap by either assuming that data at the right semantic level is available, or they develop bespoke methods for capturing such data. Our recent Scene Graph Safety Monitoring (SGSM) framework addresses this challenge by extracting scene graphs (SGs) from sensor inputs to capture the entities related to the AV, specifying driving properties using a domain-specific language that enables building propositions over those graphs and composing them through temporal logic, and synthesizing monitors to detect property violations. Through this paper we further explain, formalize, analyze, and extend the SGSM framework, producing SGSM++. This extension is significant in that it incorporates the ability for the framework to encode the semantics of resetting a property violation, enabling the framework to count the quantity and duration of violations. We implemented SGSM++ to monitor for violations of 9 properties of 3 AVs from the CARLA Autonomous Driving Leaderboard, confirming the viability of the framework, which found that the AVs violated 71% of properties during at least one test including almost 1400 unique violations over 30 total test executions, with violations lasting up to 9.25 minutes. Artifact available at https://github.com/less-lab-uva/ExtendingSGSM.	Trey Woodlief, Felipe Toledo, Sebastian Elbaum, Matthew B. Dwyer, The SGSM framework: Enabling the specification and monitor synthesis of safe driving properties through scene graphs, Science of Computer Programming, Volume 242, 2025, 103252, ISSN 0167-6423, https://doi.org/10.1016/j.scico.2024.103252.	sgsm-framework	https://authors.elsevier.com/a/1kEgNc7X5A%7EFU	"@article{WOODLIEF2025103252,
title = {The SGSM framework: Enabling the specification and monitor synthesis of safe driving properties through scene graphs},
journal = {Science of Computer Programming},
volume = {242},
pages = {103252},
year = {2025},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2024.103252},
url = {https://www.sciencedirect.com/science/article/pii/S0167642324001758},
author = {Trey Woodlief and Felipe Toledo and Sebastian Elbaum and Matthew B. Dwyer},
keywords = {Runtime verification, Autonomous vehicles, Safe driving properties, Scene graphs},
}"	/images/extending_sgsm.png	https://github.com/less-lab-uva/ExtendingSGSM	Journal